import pandas as pd
from nltk import word_tokenize
import json
import gensim
from gensim.models.word2vec import Word2Vec

##used to create word embeddings for each dataset

EMBEDDING_SIZE = 100

token_asts = []
token_cgs = []
token_sysevrs = []

# asts_path = "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\asts\\tokens_for_word2vec_asts.json"
# asts_path2 = './data/asts/all_asts.csv'
# code_gadget_path = "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\code_gadgets\\tokens_for_word2vec_code_gadgets.csv"
sysevr_path = "your path"  # "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\sysevr\\all_standardized_sysevr.csv"

# AST for words
# asts_data = open(asts_path, encoding='utf-8')
# asts_data = json.load(asts_data)

# AST for ints
# ast_df = pd.read_csv(asts_path2)
# asts_data = ast_df['vectors'].to_list()

# CODE GADGET
# code_gadget_data = pd.read_csv(code_gadget_path)
# code_gadget_data = code_gadget_data['code'].to_list()

# SYSEVR
sysevr_data = pd.read_csv(sysevr_path)
sysevr_data = sysevr_data['code'].to_list()

# print("ASTS: "+asts_data[0])
# print("CG: "+code_gadget_data[0])
# print("SYSEVR : "+sysevr_data[0])

####################################
# print('tokenizing asts')
# for adata in asts_data:
#     token = adata.split(',')
#     token_asts.append(token)
# print('tokenized asts')
# print(token_asts[0])

# print('tokenizing cgs')
# for cdata in code_gadget_data:
#     token = word_tokenize(cdata)
#     token_cgs.append(token)
# print('tokenized cgs')

print('tokenizing sysevrs')
for sdata in sysevr_data:
    token = word_tokenize(sdata)
    token_sysevrs.append(token)
print('tokenized sysevrs')
#######################################################
print("training...")

# Create AST word embeddings
# asts_w2v_model = gensim.models.Word2Vec(sentences=token_asts, min_count=1, size=EMBEDDING_SIZE, window=5, workers=5,
#                                         iter=50)
# asts_w2v_model_path = ".\\Word2Vec\\ast_num_w2v_500.txt"
# asts_w2v_model.wv.save_word2vec_format(asts_w2v_model_path, binary=False)
# print('done with asts')

# Create Code Gagdet word embeddings
# cg_w2v_model = gensim.models.Word2Vec(sentences=token_cgs, min_count=1, size=EMBEDDING_SIZE, window=5, workers=5,
#                                       iter=50)
# cg_w2v_model_path = ".\\Word2Vec\\cg_w2v_300.txt"
# cg_w2v_model.wv.save_word2vec_format(cg_w2v_model_path, binary=False)
# print('done with cgs')

# Create Sysevr word embeddings
sysevr_w2v_model = gensim.models.Word2Vec(sentences=token_sysevrs, min_count=1, size=EMBEDDING_SIZE, window=5,
                                          workers=10, iter=50)
# path to save model
sysevr_model_path = "path to store saved model."  # ".\\Word2Vec\\sysevr_w2v_300.txt"
sysevr_w2v_model.wv.save_word2vec_format(sysevr_model_path, binary=False)
print('done with sysevrs')
#
print('done')
