# # import nltk
#
# # nltk.download()
# # import re
# #
# # # keywords up to C11 and C++17; immutable set
# # keywords = frozenset({'__asm', '__builtin', '__cdecl', '__declspec', '__except', '__export', '__far16', '__far32',
# #                       '__fastcall', '__finally', '__import', '__inline', '__int16', '__int32', '__int64', '__int8',
# #                       '__leave', '__optlink', '__packed', '__pascal', '__stdcall', '__system', '__thread', '__try',
# #                       '__unaligned', '_asm', '_Builtin', '_Cdecl', '_declspec', '_except', '_Export', '_Far16',
# #                       '_Far32', '_Fastcall', '_finally', '_Import', '_inline', '_int16', '_int32', '_int64',
# #                       '_int8', '_leave', '_Optlink', '_Packed', '_Pascal', '_stdcall', '_System', '_try', 'alignas',
# #                       'alignof', 'and', 'and_eq', 'asm', 'auto', 'bitand', 'bitor', 'bool', 'break', 'case',
# #                       'catch', 'char', 'char16_t', 'char32_t', 'class', 'compl', 'const', 'const_cast', 'constexpr',
# #                       'continue', 'decltype', 'default', 'delete', 'do', 'double', 'dynamic_cast', 'else', 'enum',
# #                       'explicit', 'export', 'extern', 'false', 'final', 'float', 'for', 'friend', 'goto', 'if',
# #                       'inline', 'int', 'long', 'mutable', 'namespace', 'new', 'noexcept', 'not', 'not_eq', 'nullptr',
# #                       'operator', 'or', 'or_eq', 'override', 'private', 'protected', 'public', 'register',
# #                       'reinterpret_cast', 'return', 'short', 'signed', 'sizeof', 'static', 'static_assert',
# #                       'static_cast', 'struct', 'switch', 'template', 'this', 'thread_local', 'throw', 'true', 'try',
# #                       'typedef', 'typeid', 'typename', 'union', 'unsigned', 'using', 'virtual', 'void', 'volatile',
# #                       'wchar_t', 'while', 'xor', 'xor_eq', 'NULL'})
# # # holds known non-user-defined functions; immutable set
# # main_set = frozenset({'main'})
# # # arguments in main function; immutable set
# # main_args = frozenset({'argc', 'argv'})
# #
# # # input is a list of string lines
# # def clean_gadget(gadget):
# #     # dictionary; map function name to symbol name + number
# #     fun_symbols = {}
# #     # dictionary; map variable name to symbol name + number
# #     var_symbols = {}
# #
# #     fun_count = 1
# #     var_count = 1
# #
# #     # regular expression to catch multi-line comment
# #     rx_comment = re.compile('\*/\s*$')
# #     # regular expression to find function name candidates
# #     rx_fun = re.compile(r'\b([_A-Za-z]\w*)\b(?=\s*\()')
# #     # regular expression to find variable name candidates
# #     #rx_var = re.compile(r'\b([_A-Za-z]\w*)\b(?!\s*\()')
# #     rx_var = re.compile(r'\b([_A-Za-z]\w*)\b(?:(?=\s*\w+\()|(?!\s*\w+))(?!\s*\()')
# #
# #     # final cleaned gadget output to return to interface
# #     cleaned_gadget = []
# #
# #     for line in gadget:
# #         # process if not the header line and not a multi-line commented line
# #         if rx_comment.search(line) is None:
# #             # remove all string literals (keep the quotes)
# #             nostrlit_line = re.sub(r'".*?"', '""', line)
# #             # remove all character literals
# #             nocharlit_line = re.sub(r"'.*?'", "''", nostrlit_line)
# #             # replace any non-ASCII characters with empty string
# #             ascii_line = re.sub(r'[^\x00-\x7f]', r'', nocharlit_line)
# #
# #             # return, in order, all regex matches at string list; preserves order for semantics
# #             user_fun = rx_fun.findall(ascii_line)
# #             user_var = rx_var.findall(ascii_line)
# #
# #             # Could easily make a "clean gadget" type class to prevent duplicate functionality
# #             # of creating/comparing symbol names for functions and variables in much the same way.
# #             # The comparison frozenset, symbol dictionaries, and counters would be class scope.
# #             # So would only need to pass a string list and a string literal for symbol names to
# #             # another function.
# #             for fun_name in user_fun:
# #                 if len({fun_name}.difference(main_set)) != 0 and len({fun_name}.difference(keywords)) != 0:
# #                     # DEBUG
# #                     #print('comparing ' + str(fun_name + ' to ' + str(main_set)))
# #                     #print(fun_name + ' diff len from main is ' + str(len({fun_name}.difference(main_set))))
# #                     #print('comparing ' + str(fun_name + ' to ' + str(keywords)))
# #                     #print(fun_name + ' diff len from keywords is ' + str(len({fun_name}.difference(keywords))))
# #                     ###
# #                     # check to see if function name already in dictionary
# #                     if fun_name not in fun_symbols.keys():
# #                         fun_symbols[fun_name] = 'FUN' + str(fun_count)
# #                         fun_count += 1
# #                     # ensure that only function name gets replaced (no variable name with same
# #                     # identifier); uses positive lookforward
# #                     ascii_line = re.sub(r'\b(' + fun_name + r')\b(?=\s*\()', fun_symbols[fun_name], ascii_line)
# #
# #             for var_name in user_var:
# #                 # next line is the nuanced difference between fun_name and var_name
# #                 if len({var_name}.difference(keywords)) != 0 and len({var_name}.difference(main_args)) != 0:
# #                     # DEBUG
# #                     #print('comparing ' + str(var_name + ' to ' + str(keywords)))
# #                     #print(var_name + ' diff len from keywords is ' + str(len({var_name}.difference(keywords))))
# #                     #print('comparing ' + str(var_name + ' to ' + str(main_args)))
# #                     #print(var_name + ' diff len from main args is ' + str(len({var_name}.difference(main_args))))
# #                     ###
# #                     # check to see if variable name already in dictionary
# #                     if var_name not in var_symbols.keys():
# #                         var_symbols[var_name] = 'VAR' + str(var_count)
# #                         var_count += 1
# #                     # ensure that only variable name gets replaced (no function name with same
# #                     # identifier); uses negative lookforward
# #                     ascii_line = re.sub(r'\b(' + var_name + r')\b(?:(?=\s*\w+\()|(?!\s*\w+))(?!\s*\()', \
# #                                         var_symbols[var_name], ascii_line)
# #
# #             cleaned_gadget.append(ascii_line)
# #     # return the list of cleaned lines
# #     return cleaned_gadget
# #
# # if __name__ == '__main__':
# #     test_gadget = ["""231 151712/shm_setup.c inputfunc 11,
# #                    int main(int argc, char **argv) {,
# #                    while ((c = getopt(argc, argv, "k:s:m:o:h")) != -1) {,
# #                    switch(c) {"""]
# #
# #     test_gadget2 = ['278 151587/ffmpeg.c inputfunc 3159', 'int main(int argc,char **argv)',
# #                     'parse_loglevel(argc,argv,options);', 'if (argc > 1 && !strcmp(argv[1],"-d")) {',
# #                     'argc--;', 'argv++;', 'show_banner(argc,argv,options);',
# #                     'ret = ffmpeg_parse_options(argc,argv);', 'if (ret < 0) {']
# #
# #     test_gadget3 = ['invalid_memory_access_012_s_001 *s;',
# #                     's = (invalid_memory_access_012_s_001 *)calloc(1,sizeof(invalid_memory_access_012_s_001));',
# #                     's->a = 20;', 's->b = 20;', 's->uninit = 20;', 'free(s);]']
# #
# #     test_gadgetline = ['function(File file, Buffer buff)', 'this is a comment test */']
# #
# #     split_test = 'printf ( " " , variable ++  )'.split()
# #
# #     print(clean_gadget(test_gadget))
# #     # print(clean_gadget(test_gadget2))
# #     # print(clean_gadget(test_gadget3))
# #     # print(clean_gadget(test_gadgetline))
# #     # print(split_test)
# # from nltk.tokenize import word_tokenize
# # import json
# #
# # path = "D:\\lenovo\\Desktop\\2018280142\\function_representation_learning\\Processed_Data\\asts\\all_textual vectors.txt"
# # f = open(path, 'r', encoding='utf-8')
# #
# # text_array = []
# #
# # for line in f:
# #     raw_line = line
# #     strip_line = raw_line.strip()
# #     split_line = strip_line.split('txt,')
# #     new_line = split_line[1:]
# #     # print(new_line)
# #
# #     if len(new_line) > 0:
# #         text_array.append(new_line)
# #
# # final_arr = []
# #
# # for txt in text_array:
# #     joined = "".join(txt)
# #     final_arr.append(joined)
# #
# # token = word_tokenize(final_arr[0])
# #
# # js_code = json.dumps(final_arr)
# # path = "D:\\lenovo\\Desktop\\tokens_for_word2vec_asts.json"
# # f = open(path, 'w', encoding='utf-8')
# # f.write(js_code)
# # f.close()
# # print(len(text_array))
# # print(text_array[0])
#
# import gensim
# from keras import backend as K
# K.tensorflow_backend._get_available_gpus()
# # genrate word2vec
# import pandas as pd
# from gensim.models import KeyedVectors, Word2Vec
# import json
#
# EMBEDDING_SIZE = 300
#
# token_asts = []
# final_asts = []
# path = 'D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\asts\\all_asts.csv'
#
# df = pd.read_csv(path)
#
# num_vectors = df['vectors'].to_list()
#
# size = len(num_vectors)
# print(size)
#
# for line in num_vectors:
#     strip_line = line.strip()
#     token = strip_line.split(",")
#     final_asts.append(token)
#
# w2v_model = gensim.models.Word2Vec(sentences=final_asts, min_count=1, size=EMBEDDING_SIZE, window=5, workers=5, iter=50)
# w2v_model_path = ".\\Word2Vec\\ast_num_w2v_300.txt"
# w2v_model_path2 = ".\\Word2Vec\\ast_w2v_300.txt"
# w2v_model.wv.save_word2vec_format(w2v_model_path, binary=False)
#
# print('w2v saved')
#
# ast_model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=False)
# print(ast_model.wv.most_similar('45'))
#
# ast_model2 = KeyedVectors.load_word2vec_format(w2v_model_path2, binary=False)
# # print(ast_model.wv.most_similar('45'))
#
# ast_words_path = ".\\Word2Vec\\ast_num_vocab_300.json"
# ast_words = list(ast_model.wv.vocab)
# print(ast_words[0:5])
#
# ast_words_path2 = ".\\Word2Vec\\ast_vocab_300.json"
# ast_words2 = list(ast_model2.wv.vocab)
# print(ast_words2[0:5])
#
# with open(ast_words_path, 'w') as f1:
#     json.dump(ast_words, f1)
#
# with open(ast_words_path2, 'w') as f2:
#     json.dump(ast_words2, f2)
#
# #run preprocess_syssevr
# from sysevr import preprocess_sysevr as psvr
# import pandas as pd
# import time
# from nltk import word_tokenize
#
#
# path = ".\\data\\sysevr\\all_sysevr.csv"
# sysevr_df = pd.read_csv(path)
#
# code = sysevr_df["code"].to_list()
# labels = sysevr_df["label"].to_list()
# print(len(code))
# # test_data = code[0:10]
# # test_data = code[0:10]
# # index=6
# # print("before: " + test_data[index])
# #
# sstart = time.time()
#
# print('standardizing sysevr')
# standard_sysevr = psvr.mapping(code)
# #
# # print("after: " + new_data[index])
# #
# # token = new_data[index]
# # nt = word_tokenize(token)
# print('done')
#
# ssend = time.time() - sstart
# print(ssend)
#
# wtime = time.time()
# print('writting to file')
# allsysevr = []
# for i in range(0, len(standard_sysevr)):
#     ftext = standard_sysevr[i]
#     flabel = labels[i]
#     allsysevr.append([ftext, flabel])
#
# vdf = pd.DataFrame.from_records(allsysevr, columns=['code', 'label'])
# print(vdf.head(5))
#
# path1 = ".\\data\\sysevr\\all_standardized_sysevr.csv"
# vdf.to_csv(path1, index=None, header=True)
# print("done")
#
# print(time.time()-wtime)
#
#
#
# import nltk

# nltk.download()
# import re
#
# # keywords up to C11 and C++17; immutable set
# keywords = frozenset({'__asm', '__builtin', '__cdecl', '__declspec', '__except', '__export', '__far16', '__far32',
#                       '__fastcall', '__finally', '__import', '__inline', '__int16', '__int32', '__int64', '__int8',
#                       '__leave', '__optlink', '__packed', '__pascal', '__stdcall', '__system', '__thread', '__try',
#                       '__unaligned', '_asm', '_Builtin', '_Cdecl', '_declspec', '_except', '_Export', '_Far16',
#                       '_Far32', '_Fastcall', '_finally', '_Import', '_inline', '_int16', '_int32', '_int64',
#                       '_int8', '_leave', '_Optlink', '_Packed', '_Pascal', '_stdcall', '_System', '_try', 'alignas',
#                       'alignof', 'and', 'and_eq', 'asm', 'auto', 'bitand', 'bitor', 'bool', 'break', 'case',
#                       'catch', 'char', 'char16_t', 'char32_t', 'class', 'compl', 'const', 'const_cast', 'constexpr',
#                       'continue', 'decltype', 'default', 'delete', 'do', 'double', 'dynamic_cast', 'else', 'enum',
#                       'explicit', 'export', 'extern', 'false', 'final', 'float', 'for', 'friend', 'goto', 'if',
#                       'inline', 'int', 'long', 'mutable', 'namespace', 'new', 'noexcept', 'not', 'not_eq', 'nullptr',
#                       'operator', 'or', 'or_eq', 'override', 'private', 'protected', 'public', 'register',
#                       'reinterpret_cast', 'return', 'short', 'signed', 'sizeof', 'static', 'static_assert',
#                       'static_cast', 'struct', 'switch', 'template', 'this', 'thread_local', 'throw', 'true', 'try',
#                       'typedef', 'typeid', 'typename', 'union', 'unsigned', 'using', 'virtual', 'void', 'volatile',
#                       'wchar_t', 'while', 'xor', 'xor_eq', 'NULL'})
# # holds known non-user-defined functions; immutable set
# main_set = frozenset({'main'})
# # arguments in main function; immutable set
# main_args = frozenset({'argc', 'argv'})
#
# # input is a list of string lines
# def clean_gadget(gadget):
#     # dictionary; map function name to symbol name + number
#     fun_symbols = {}
#     # dictionary; map variable name to symbol name + number
#     var_symbols = {}
#
#     fun_count = 1
#     var_count = 1
#
#     # regular expression to catch multi-line comment
#     rx_comment = re.compile('\*/\s*$')
#     # regular expression to find function name candidates
#     rx_fun = re.compile(r'\b([_A-Za-z]\w*)\b(?=\s*\()')
#     # regular expression to find variable name candidates
#     #rx_var = re.compile(r'\b([_A-Za-z]\w*)\b(?!\s*\()')
#     rx_var = re.compile(r'\b([_A-Za-z]\w*)\b(?:(?=\s*\w+\()|(?!\s*\w+))(?!\s*\()')
#
#     # final cleaned gadget output to return to interface
#     cleaned_gadget = []
#
#     for line in gadget:
#         # process if not the header line and not a multi-line commented line
#         if rx_comment.search(line) is None:
#             # remove all string literals (keep the quotes)
#             nostrlit_line = re.sub(r'".*?"', '""', line)
#             # remove all character literals
#             nocharlit_line = re.sub(r"'.*?'", "''", nostrlit_line)
#             # replace any non-ASCII characters with empty string
#             ascii_line = re.sub(r'[^\x00-\x7f]', r'', nocharlit_line)
#
#             # return, in order, all regex matches at string list; preserves order for semantics
#             user_fun = rx_fun.findall(ascii_line)
#             user_var = rx_var.findall(ascii_line)
#
#             # Could easily make a "clean gadget" type class to prevent duplicate functionality
#             # of creating/comparing symbol names for functions and variables in much the same way.
#             # The comparison frozenset, symbol dictionaries, and counters would be class scope.
#             # So would only need to pass a string list and a string literal for symbol names to
#             # another function.
#             for fun_name in user_fun:
#                 if len({fun_name}.difference(main_set)) != 0 and len({fun_name}.difference(keywords)) != 0:
#                     # DEBUG
#                     #print('comparing ' + str(fun_name + ' to ' + str(main_set)))
#                     #print(fun_name + ' diff len from main is ' + str(len({fun_name}.difference(main_set))))
#                     #print('comparing ' + str(fun_name + ' to ' + str(keywords)))
#                     #print(fun_name + ' diff len from keywords is ' + str(len({fun_name}.difference(keywords))))
#                     ###
#                     # check to see if function name already in dictionary
#                     if fun_name not in fun_symbols.keys():
#                         fun_symbols[fun_name] = 'FUN' + str(fun_count)
#                         fun_count += 1
#                     # ensure that only function name gets replaced (no variable name with same
#                     # identifier); uses positive lookforward
#                     ascii_line = re.sub(r'\b(' + fun_name + r')\b(?=\s*\()', fun_symbols[fun_name], ascii_line)
#
#             for var_name in user_var:
#                 # next line is the nuanced difference between fun_name and var_name
#                 if len({var_name}.difference(keywords)) != 0 and len({var_name}.difference(main_args)) != 0:
#                     # DEBUG
#                     #print('comparing ' + str(var_name + ' to ' + str(keywords)))
#                     #print(var_name + ' diff len from keywords is ' + str(len({var_name}.difference(keywords))))
#                     #print('comparing ' + str(var_name + ' to ' + str(main_args)))
#                     #print(var_name + ' diff len from main args is ' + str(len({var_name}.difference(main_args))))
#                     ###
#                     # check to see if variable name already in dictionary
#                     if var_name not in var_symbols.keys():
#                         var_symbols[var_name] = 'VAR' + str(var_count)
#                         var_count += 1
#                     # ensure that only variable name gets replaced (no function name with same
#                     # identifier); uses negative lookforward
#                     ascii_line = re.sub(r'\b(' + var_name + r')\b(?:(?=\s*\w+\()|(?!\s*\w+))(?!\s*\()', \
#                                         var_symbols[var_name], ascii_line)
#
#             cleaned_gadget.append(ascii_line)
#     # return the list of cleaned lines
#     return cleaned_gadget
#
# if __name__ == '__main__':
#     test_gadget = ["""231 151712/shm_setup.c inputfunc 11,
#                    int main(int argc, char **argv) {,
#                    while ((c = getopt(argc, argv, "k:s:m:o:h")) != -1) {,
#                    switch(c) {"""]
#
#     test_gadget2 = ['278 151587/ffmpeg.c inputfunc 3159', 'int main(int argc,char **argv)',
#                     'parse_loglevel(argc,argv,options);', 'if (argc > 1 && !strcmp(argv[1],"-d")) {',
#                     'argc--;', 'argv++;', 'show_banner(argc,argv,options);',
#                     'ret = ffmpeg_parse_options(argc,argv);', 'if (ret < 0) {']
#
#     test_gadget3 = ['invalid_memory_access_012_s_001 *s;',
#                     's = (invalid_memory_access_012_s_001 *)calloc(1,sizeof(invalid_memory_access_012_s_001));',
#                     's->a = 20;', 's->b = 20;', 's->uninit = 20;', 'free(s);]']
#
#     test_gadgetline = ['function(File file, Buffer buff)', 'this is a comment test */']
#
#     split_test = 'printf ( " " , variable ++  )'.split()
#
#     print(clean_gadget(test_gadget))
#     # print(clean_gadget(test_gadget2))
#     # print(clean_gadget(test_gadget3))
#     # print(clean_gadget(test_gadgetline))
#     # print(split_test)
# from nltk.tokenize import word_tokenize
# import json
#
# path = "D:\\lenovo\\Desktop\\2018280142\\function_representation_learning\\Processed_Data\\asts\\all_textual vectors.txt"
# f = open(path, 'r', encoding='utf-8')
#
# text_array = []
#
# for line in f:
#     raw_line = line
#     strip_line = raw_line.strip()
#     split_line = strip_line.split('txt,')
#     new_line = split_line[1:]
#     # print(new_line)
#
#     if len(new_line) > 0:
#         text_array.append(new_line)
#
# final_arr = []
#
# for txt in text_array:
#     joined = "".join(txt)
#     final_arr.append(joined)
#
# token = word_tokenize(final_arr[0])
#
# js_code = json.dumps(final_arr)
# path = "D:\\lenovo\\Desktop\\tokens_for_word2vec_asts.json"
# f = open(path, 'w', encoding='utf-8')
# f.write(js_code)
# f.close()
# print(len(text_array))
# print(text_array[0])

# import gensim
# from keras import backend as K
# K.tensorflow_backend._get_available_gpus()
# genrate word2vec
# import pandas as pd
# from gensim.models import KeyedVectors, Word2Vec
# import json
#
# EMBEDDING_SIZE = 100

# token_asts = []
# final_asts = []
# path = 'D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\asts\\all_asts.csv'
#
# df = pd.read_csv(path)
#
# num_vectors = df['vectors'].to_list()
#
# size = len(num_vectors)
# print(size)
#
# for line in num_vectors:
#     strip_line = line.strip()
#     token = strip_line.split(",")
#     final_asts.append(token)
#
# w2v_model = gensim.models.Word2Vec(sentences=final_asts, min_count=1, size=EMBEDDING_SIZE, window=5, workers=5, iter=50)
# w2v_model_path = ".\\Word2Vec\\ast_num_w2v.txt"
# w2v_model_path2 = ".\\Word2Vec\\ast_w2v.txt"
# w2v_model.wv.save_word2vec_format(w2v_model_path, binary=False)
#
# print('w2v saved')
#
# ast_model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=False)
# print(ast_model.wv.most_similar('45'))
#
# ast_model2 = KeyedVectors.load_word2vec_format(w2v_model_path2, binary=False)
# # print(ast_model.wv.most_similar('45'))
#
# ast_words_path = ".\\Word2Vec\\ast_num_vocab.json"
# ast_words = list(ast_model.wv.vocab)
# print(ast_words[0:5])
#
# ast_words_path2 = ".\\Word2Vec\\ast_vocab.json"
# ast_words2 = list(ast_model2.wv.vocab)
# print(ast_words2[0:5])
#
# with open(ast_words_path, 'w') as f1:
#     json.dump(ast_words, f1)
#
# with open(ast_words_path2, 'w') as f2:
#     json.dump(ast_words2, f2)

# run preprocess_syssevr
# import pandas as pd
# from nltk import word_tokenize
# import gensim
# from gensim.models.word2vec import Word2Vec
# from gensim.models import KeyedVectors
# import json
#
# EMBEDDING_SIZE = 100
#
# path = ".\\data\\sysevr\\all_standardized_sysevr.csv"
#
# sysevr_df = pd.read_csv(path)
# syse_vr_tokens = []
#
# code = sysevr_df['code'].tolist()
# i = 0
# print('tokenizing')
# for c in code:
#     i += 1
#     print(i)
#     token = word_tokenize(c)
#     syse_vr_tokens.append(token)
# print('tokenizing done')
#
# print('training')
# #Create word embeddings
# w2v_model = gensim.models.Word2Vec(sentences=syse_vr_tokens, min_count=1, size=EMBEDDING_SIZE, window=5, workers=10, iter=50)
# w2v_model_path = ".\\Word2Vec\\sysevr_w2v.txt"
# w2v_model.wv.save_word2vec_format(w2v_model_path, binary=False)
# print('done training')
#
# model = KeyedVectors.load_word2vec_format(w2v_model_path, binary=False)
# print(model.wv.most_similar('int'))
#
# sysevr_words_path = ".\\Word2Vec\\sysevr_vocab.json"
# sysevr_words = list(model.wv.vocab)
# print(sysevr_words[0:5])
#
# with open(sysevr_words_path, 'w') as f1:
#     json.dump(sysevr_words, f1)
#
# print('done')

# import pandas as pd
# import numpy as np
#
# cg_path = "./data/code_gadgets/all_code_gadgets.csv"
#
# cg_df = pd.read_csv(cg_path)
# cg_data = cg_df['code'].to_list()
# cg_labels = cg_df['label'].to_list()
#
# back_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 1))]
# forward_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 0))]
#
# back_slices_data = back_slices_df['code'].tolist()
# forward_slices_data = forward_slices_df['code'].tolist()
#
# print(back_slices_data[0])
# print(forward_slices_data[0])


# import numpy as np

# a = ['1', '2', '3', '4', '5']
# b = [1, 2, 3, 4, 5]
#
# data = np.asarray(a)
# classes = np.asarray(b)
#
# idx = np.random.permutation(len(data))
# a, b = data[idx], classes[idx]
#
# print(a)
# print(b)
# #
# # Driver code
# dict1 = {'a': 1, 'b': 2}
# dict2 = {'c': 3, 'd': 4}
# dict3 = {**dict1, **dict2}
#
# print(dict3)

# import numpy as np
# from sklearn.metrics import confusion_matrix
# from sklearn.metrics import classification_report
# from keras.utils.np_utils import to_categorical
#
# y_true = [1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]
# oh_ytrue = to_categorical(y_true)
# y_pred = [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]
#
# res = classification_report(oh_ytrue.argmax(axis=1), y_pred, target_names=["0", "1"])
# tn, fp, fn, tp = confusion_matrix(oh_ytrue.argmax(axis=1), y_pred).ravel()
#
# fpr = 'False positive rate: ' + str(fp / (fp + tn)) + ""
#
# fnr = 'False negative rate: ' + str(fn / (fn + tp)) + ""
#
# recall = tp / (tp + fn)
# rcl = 'Recall: ' + str(recall) + ""
#
# precision = tp / (tp + fp)
# pr = 'Precision: ' + str(precision) + ""
#
# f1_score = ((2 * precision * recall) / (precision + recall))
# f1 = 'F1 score: ' + str(f1_score) + ""
#
# print(res)
# print(fpr)
# print(fnr)
# print(rcl)
# print(pr)
# print(f1)

# title = "san"+"\n"
# path = ".\\graphs_reports\\tt.txt"
# f = open(path, "w")
# f.write(title+np.array_str(res))
# f.close()

# import numpy as np
# from sklearn.metrics import classification_report

#
# y_true = [0, 1, 2, 2, 2, 0, 2, 1]
# y_pred = [0, 0, 2, 2, 1, 0, 2, 1]
# target_names = ['class 0', 'class 1', 'class 2']
# res = classification_report(y_true, y_pred, target_names=target_names)
# print(res)
# path = ".\\graphs_reports\\tt.txt"
# f = open(path, "w")
# f.write("title"+"\n"+res)
# f.close()

# import pandas as pd
#
# asts_path = ".\\data\\asts\\all_asts.csv"
#
# ast_df = pd.read_csv(asts_path)
# ast_data = ast_df['vectors'].to_list()
# ast_labels = ast_df['label'].to_list()
# #
# ast_nv_data = ast_df.loc[ast_df['label'] == 0]
# ast_v_data = ast_df.loc[ast_df['label'] == 1]
#
# ast_nv_data_list = ast_nv_data['vectors'].tolist()
# ast_v_data_list = ast_v_data['vectors'].tolist()
#
# print(len(ast_nv_data_list))
# print(len(ast_v_data_list))

from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score

y_true = [0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0]
y_pred = [0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1]

print(accuracy_score(y_true, y_pred))
print('')
print(precision_score(y_true, y_pred, average='binary'))
print('')
print(recall_score(y_true, y_pred, average='binary'))
print('')
print(f1_score(y_true, y_pred, average='binary'))
# print(precision_score(y_true, y_pred, average='micro'))
#
# print(precision_score(y_true, y_pred, average='weighted'))
