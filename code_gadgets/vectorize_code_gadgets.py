import nltk
# nltk.download()
import json
import pandas as pd

path0 = "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\code_gadgets\\standardized_code_gadgets.json"
path1 = "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\code_gadgets\\standardized_code_gadgets_labels.json"

operators3 = {'<<=', '>>='}
operators2 = {
    '->', '++', '--',
    '!~', '<<', '>>', '<=', '>=',
    '==', '!=', '&&', '||', '+=',
    '-=', '*=', '/=', '%=', '&=', '^=', '|='
}
operators1 = {
    '(', ')', '[', ']', '.',
    '+', '-', '*', '&', '/',
    '%', '<', '>', '^', '|',
    '=', ',', '?', ':', ';',
    '{', '}'
}

file_data = open(path0, encoding='utf-8')
file_data1 = open(path1, encoding='utf-8')

json_data = json.load(file_data)
json_labels = json.load(file_data1)

# for line in json_data:
arr = []

test_data = json_data

for i in range(len(test_data)):
    gadget = json_data[i]
    tt = ''

    for l in gadget:
        tt += l
    arr.append(tt)

# print(arr[0])
# print(arr[1])
# print(arr[2])
# print(len(arr))
# print(arr[652])
# for label in json_labels:
# print(label)
# token = word_tokenize(arr[2])
# print(token)
print("done")
print(len(arr))
print(json_labels[0])

# token = word_tokenize(arr[0])
# print(token)
tokens_array = []

for i in range(0, len(arr)):
    text = arr[i]
    label = json_labels[i]
    tokens_array.append([text, label])

vdf = pd.DataFrame.from_records(tokens_array, columns=['code', 'label'])
print(vdf.head(5))
path = "D:\\lenovo\\Desktop\\2018280142\\Preprocess\\data\\code_gadgets"
vdf.to_csv(path, index=None, header=True)
# # gadget = json_data[0]
# # tt = ''
# # for l in gadget:
# #     tt += l
#
# # print(tt)
#
# # gadget = """int main(int argc,char **argv)
# # FUN1(argc,argv,VAR2);
# # if (argc > 1 && !FUN2(argv[1],""))
# # {argc--; argv++;
# # FUN3(argc,argv,VAR2);
# # VAR3 = FUN4(argc,argv);
# # if (VAR3 < 0) {
# # """
#
# # print(gadget)
#
# # print("Before: ")
# # print(gadget)
# # token = word_tokenize(tt)
# # print("After: ")
# # print(token)
#
# # tokenized = []
# # function_regex = re.compile('FUN(\d)+')
# # backwards_slice = False
#
# # for line in test_data:
# #     tokens = GadgetVectorizer.tokenize(line)
# #     tokenized += tokens
# #     # tok.append(tokenized)
# #     if len(list(filter(function_regex.match, tokens))) > 0:
# #         backwards_slice = True
# #     else:
# #         backwards_slice = False
# #
# # print(tokenized)
# # # print(backwards_

