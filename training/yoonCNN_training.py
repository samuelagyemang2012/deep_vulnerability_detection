import numpy as np  # linear algebra
import pandas as pd
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
from Models import hyperparameters, my_preprocessing
from Models.models import YoonCNN, metrics_to_file, acc_loss_graphs_to_file, create_callbacks

CG_MAX = 50
AST_MAX = 620
SYSEVR_MAX = 500
EMBEDDING_SIZE = 100
PATIENCE = 10
VAL_SPLIT = 0.1
SPLIT_TEST_SIZE = 0.2

EPOCHS = hyperparameters.EPOCHS
BATCH_SIZE = hyperparameters.BATCH_SIZE
OPTIMIZER = hyperparameters.OPTIMIZER
TRAINING_SET = '1'

# Set data paths
asts_path = "../data/asts/all_asts.csv"
cg_path = "../data/code_gadgets/all_code_gadgets.csv"
sysevr_path = "../data/sysevr/all_standardized_sysevr.csv"

# Load word embeddings
asts_w2v_path = "..\\Word2Vec\\ast_num_w2v.txt"
asts_vocab = "..\\Word2Vec\\ast_num_vocab.json"

cg_w2v_path = "..\\Word2Vec\\cg_w2v.txt"
cg_vocab = "..\\Word2Vec\\cg_vocab.json"

sysevr_w2v_path = "..\\Word2Vec\\sysevr_w2v.txt"
sysevr_vocab = "..\\Word2Vec\\sysevr_vocab.json"

# Load data
# ASTs
ast_df = pd.read_csv(asts_path)
ast_data = ast_df['vectors'].to_list()
ast_labels = ast_df['label'].to_list()

##########################################
# Code cadgets
cg_df = pd.read_csv(cg_path)
cg_data = cg_df['code'].to_list()
# cg_labels = cg_df['label'].to_list()

back_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 1))]
forward_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 0))]

back_slices_data = back_slices_df['code'].tolist()
forward_slices_data = forward_slices_df['code'].tolist()
back_slices_labels = back_slices_df['label'].tolist()
forward_slices_labels = forward_slices_df['label'].tolist()

################################################
# Sysevr
sysevr_df = pd.read_csv(sysevr_path)
sysevr_data = sysevr_df['code'].to_list()
sysevr_labels = sysevr_df['label'].to_list()
sysevr_focus = sysevr_df['focus'].to_list()

# # Get max number of words in each dataset
# ast_max_length = max(len(ast.split(",")) for ast in ast_data)  # 5823
# cg_max_length = max(len(cg.split()) for cg in cg_data)  # 1516
# sysevr_max_length = max(len(sysevr.split()) for sysevr in sysevr_data)  # 966
#
# print("AST max :" + str(ast_max_length))
# print("Code Gadget max :" + str(cg_max_length))
# print("Sysevr max :" + str(sysevr_max_length))

# Load word embeddings

print('loading word embeddings')

ast_embeddings = open(asts_w2v_path, encoding='utf-8')
with open(asts_vocab, 'r') as a:
    words1 = a.read()
ast_words = json.loads(words1)
print('fetched ast embeddings')

cg_embeddings = open(cg_w2v_path, encoding='utf-8')
with open(cg_vocab, 'r') as b:
    words2 = b.read()
cg_words = json.loads(words2)
print('fetched cg embeddings')

sysevr_embeddings = open(sysevr_w2v_path, encoding='utf-8')
with open(sysevr_vocab, 'r') as c:
    words3 = c.read()
sysevr_words = json.loads(words3)
print('fetched sysevr embeddings')

# Create embedding dictionary
ast_emb_dict = {}
for line in ast_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        ast_emb_dict[word] = vectors
ast_embeddings.close()

cg_emb_dict = {}
for line in cg_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        cg_emb_dict[word] = vectors
cg_embeddings.close()

sysevr_emb_dict = {}
for line in sysevr_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        sysevr_emb_dict[word] = vectors
sysevr_embeddings.close()

print('asts embedding')
print(ast_emb_dict['34'])

print('cg embedding')
print(cg_emb_dict['VAR1'])

print('sysevr embedding')
print(sysevr_emb_dict['const'])

# Tokenize corpus
ast_tokenizer = Tokenizer()

cg_tokenizer = Tokenizer()
bcg_tokenizer = Tokenizer()
fcg_tokenizer = Tokenizer()

sysevr_tokenizer = Tokenizer()

print("tokenizing asts")
# Fit tokenizers
ast_tokenizer.fit_on_texts(ast_data)

print("tokenizing cgs")
bcg_tokenizer.fit_on_texts(back_slices_data)
fcg_tokenizer.fit_on_texts(forward_slices_data)

print("tokenizing sysevr")
sysevr_tokenizer.fit_on_texts(sysevr_data)

#################################################
print("creating ast sequence")
ast_sequences = my_preprocessing.ast_sequence(ast_data)

print("creating cg sequence")
bcg_sequences = bcg_tokenizer.texts_to_sequences(back_slices_data)
fcg_sequences = fcg_tokenizer.texts_to_sequences(forward_slices_data)

print("creating sysevr sequence")
sysevr_sequences = sysevr_tokenizer.texts_to_sequences(sysevr_data)

# Pad sequences to the same length
# pad asts
print("padding asts")
ast_padded_data = pad_sequences(ast_sequences, maxlen=AST_MAX, padding='post')

# pad code gadgets
print("padding cgs")
bcg_padded_data = pad_sequences(bcg_sequences, maxlen=CG_MAX, padding='pre')
fcg_padded_data = pad_sequences(fcg_sequences, maxlen=CG_MAX, padding='post')
cg_padded_data = np.concatenate((bcg_padded_data, fcg_padded_data), axis=0)

# pad sysevr
print("padding sysevr")

sysevr_padded_data = my_preprocessing.sevc_pad_sequences(sysevr_sequences, sysevr_focus, SYSEVR_MAX)

#################
# covert all labels to numpy arrays
ast_labels_np = np.asarray(ast_labels)

back_slices_labels_np = np.asarray(back_slices_labels)
forward_slices_labels_np = np.asarray(forward_slices_labels)
cg_labels_np = np.concatenate((back_slices_labels_np, forward_slices_labels_np), axis=0)

sysevr_labels_np = np.asarray(sysevr_labels)

print("shuffle data")
# shuffle ast gadgets
ids = np.random.permutation(len(ast_padded_data))
n_ast_padded_data, n_ast_labels = ast_padded_data[ids], ast_labels_np[ids]

# shuffle code gadgets
idx = np.random.permutation(len(cg_padded_data))
n_cg_padded_data, n_cg_labels = cg_padded_data[idx], cg_labels_np[idx]

# shuffle sysevr
idz = np.random.permutation(len(sysevr_padded_data))
n_sysevr_padded_data, n_sysevr_labels = sysevr_padded_data[idz], sysevr_labels_np[idz]

print("creating word indices")
# Get word indices
ast_word_index = ast_tokenizer.word_index

bs_word_index = bcg_tokenizer.word_index
fs_word_index = fcg_tokenizer.word_index
cg_word_index = {**bs_word_index, **fs_word_index}

sysevr_word_index = sysevr_tokenizer.word_index

# Create embedding matrix
ast_num_words = len(ast_word_index) + 1
cg_num_words = len(cg_word_index) + 1
sysevr_num_words = len(sysevr_word_index) + 1

ast_embedding_matrix = np.zeros((ast_num_words, EMBEDDING_SIZE))
cg_embedding_matrix = np.zeros((cg_num_words, EMBEDDING_SIZE))
sysevr_embedding_matrix = np.zeros((sysevr_num_words, EMBEDDING_SIZE))

# ast embedding matrix
print('creating ast embedding matrix')
for aword, anum in ast_word_index.items():
    if anum > ast_num_words:
        continue
    ast_embedding_vector = ast_emb_dict.get(aword)
    if ast_embedding_vector is not None:
        ast_embedding_matrix[anum] = ast_embedding_vector
    else:
        ast_embedding_matrix[anum] = np.random.randn(EMBEDDING_SIZE)

# code gadget embedding matrix
print('creating cg embedding matrix')
for cgword, cgnum in cg_word_index.items():
    if cgnum > cg_num_words:
        continue
    cg_embedding_vector = cg_emb_dict.get(cgword)
    if cg_embedding_vector is not None:
        cg_embedding_matrix[cgnum] = cg_embedding_vector
    else:
        cg_embedding_matrix[cgnum] = np.random.randn(EMBEDDING_SIZE)

# sysevr embedding matrix
print('creating sysevr embedding matrix')
for sword, snum in sysevr_word_index.items():
    if snum > sysevr_num_words:
        continue
    sysevr_embedding_vector = sysevr_emb_dict.get(sword)
    if sysevr_embedding_vector is not None:
        sysevr_embedding_matrix[snum] = sysevr_embedding_vector
    else:
        sysevr_embedding_matrix[snum] = np.random.randn(EMBEDDING_SIZE)

# split data 80, 20
print('splitting data')
ast_X_train, ast_X_test, ast_y_train, ast_y_test = train_test_split(n_ast_padded_data, n_ast_labels,
                                                                    test_size=SPLIT_TEST_SIZE)
cg_X_train, cg_X_test, cg_y_train, cg_y_test = train_test_split(n_cg_padded_data, n_cg_labels,
                                                                test_size=SPLIT_TEST_SIZE)
sysevr_X_train, sysevr_X_test, sysevr_y_train, sysevr_y_test = train_test_split(n_sysevr_padded_data, n_sysevr_labels,
                                                                                test_size=SPLIT_TEST_SIZE)
print('one-hot encoding labels')
# one hot encode labels
ast_y_train = to_categorical(ast_y_train)
ast_y_test = to_categorical(ast_y_test)

cg_y_train = to_categorical(cg_y_train)
cg_y_test = to_categorical(cg_y_test)

sysevr_y_train = to_categorical(sysevr_y_train)
sysevr_y_test = to_categorical(sysevr_y_test)

# Creating early stopping callback and Model Checkpoint
print("creating callbacks")
ast_best_model_path = "..\\saved_models\\YOONCNN\\ast_yooncnn_model_set" + TRAINING_SET + ".h5"
ast_callback = create_callbacks(ast_best_model_path, 'val_loss', 'min', PATIENCE)

cg_best_model_path = "..\\saved_models\\YOONCNN\\cg_yooncnn_model_set" + TRAINING_SET + ".h5"
cg_callback = create_callbacks(cg_best_model_path, 'val_loss', 'min', PATIENCE)

sysevr_best_model_path = "..\\saved_models\\YOONCNN\\sysevr_yooncnn_model_set" + TRAINING_SET + ".h5"
sysevr_callback = create_callbacks(sysevr_best_model_path, 'val_loss', 'min', PATIENCE)

# Compile models
# ASTS
print('training ast_yooncnn')
ast_yooncnn = YoonCNN(ast_num_words, EMBEDDING_SIZE, ast_embedding_matrix, AST_MAX)
ast_yooncnn.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
ast_yooncnn.summary()

ast_yooncnn_history = ast_yooncnn.fit(ast_X_train, ast_y_train,
                                      epochs=EPOCHS,
                                      batch_size=BATCH_SIZE,
                                      verbose=1,
                                      validation_split=VAL_SPLIT,
                                      callbacks=ast_callback,
                                      shuffle=False)
ast_yooncnn_acc = ast_yooncnn.evaluate(ast_X_test, ast_y_test, batch_size=BATCH_SIZE)
ast_yooncnn_preds = ast_yooncnn.predict(ast_X_test, verbose=1)
ast_yooncnn_preds = np.argmax(ast_yooncnn_preds, axis=1)

ast_yooncnn_loss_path = "../graphs_reports/YOONCNN/AST_YOONCNN_LOSS_set" + TRAINING_SET + ".png"
ast_yooncnn_acc_path = "../graphs_reports/YOONCNN/AST_YOONCNN_ACC_set" + TRAINING_SET + ".png"
acc_loss_graphs_to_file("AST_YOONCNN-SET" + TRAINING_SET, ast_yooncnn_history, ['train', 'val'], 'upper left',
                        ast_yooncnn_loss_path,
                        ast_yooncnn_acc_path)

ast_yooncnn_metrics_path = "../graphs_reports/YOONCNN/ast_yooncnn_results_set" + TRAINING_SET + ".txt"
metrics_to_file("AST YOONCNN RESULTS-SET" + TRAINING_SET, ast_yooncnn_metrics_path, ast_y_test, ast_yooncnn_preds,
                ['0', '1'],
                ast_yooncnn_acc, BATCH_SIZE, "", OPTIMIZER)
########################################################################################################################

# CODE GADGETS
print('training cg_yooncnn')
cg_yooncnn = YoonCNN(cg_num_words, EMBEDDING_SIZE, cg_embedding_matrix, CG_MAX)
cg_yooncnn.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
cg_yooncnn.summary()

cg_yooncnn_history = cg_yooncnn.fit(cg_X_train, cg_y_train,
                                    epochs=EPOCHS,
                                    batch_size=BATCH_SIZE,
                                    verbose=1,
                                    validation_split=VAL_SPLIT,
                                    callbacks=cg_callback,
                                    shuffle=False)
cg_yooncnn_acc = cg_yooncnn.evaluate(cg_X_test, cg_y_test, batch_size=BATCH_SIZE)
cg_yooncnn_preds = cg_yooncnn.predict(cg_X_test, verbose=1)
cg_yooncnn_preds = np.argmax(cg_yooncnn_preds, axis=1)

cg_yooncnn_loss_path = "../graphs_reports/YOONCNN/CG_YOONCNN_LOSS_set" + TRAINING_SET + ".png"
cg_yooncnn_acc_path = "../graphs_reports/YOONCNN/CG_YOONCNN_ACC_set" + TRAINING_SET + ".png"
acc_loss_graphs_to_file("CG_YOONCNN-SET" + TRAINING_SET, cg_yooncnn_history, ['train', 'val'], 'upper left',
                        cg_yooncnn_loss_path,
                        cg_yooncnn_acc_path)

cg_yooncnn_metrics_path = "../graphs_reports/YOONCNN/cg_yooncnn_results_set" + TRAINING_SET + ".txt"
metrics_to_file("CG YOONCNN RESULTS-SET" + TRAINING_SET, cg_yooncnn_metrics_path, cg_y_test, cg_yooncnn_preds,
                ['0', '1'],
                cg_yooncnn_acc, BATCH_SIZE, "", OPTIMIZER)
########################################################################################################################

# SYSEVR
print('training sysevr_yooncnn')
sysevr_yooncnn = YoonCNN(sysevr_num_words, EMBEDDING_SIZE, sysevr_embedding_matrix, SYSEVR_MAX)
sysevr_yooncnn.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
sysevr_yooncnn.summary()

sysevr_yooncnn_history = sysevr_yooncnn.fit(sysevr_X_train, sysevr_y_train,
                                            epochs=EPOCHS,
                                            batch_size=BATCH_SIZE,
                                            verbose=1,
                                            validation_split=VAL_SPLIT,
                                            callbacks=sysevr_callback,
                                            shuffle=False)
sysevr_yooncnn_acc = sysevr_yooncnn.evaluate(sysevr_X_test, sysevr_y_test, batch_size=BATCH_SIZE)
sysevr_yooncnn_preds = sysevr_yooncnn.predict(sysevr_X_test, verbose=1)
sysevr_yooncnn_preds = np.argmax(sysevr_yooncnn_preds, axis=1)

sysevr_yooncnn_loss_path = "../graphs_reports/YOONCNN/SYSEVR_YOONCNN_LOSS_set" + TRAINING_SET + ".png"
sysevr_yooncnn_acc_path = "../graphs_reports/YOONCNN/SYSEVR_YOONCNN_ACC_set" + TRAINING_SET + ".png"
acc_loss_graphs_to_file("SYSEVR_YOONCNN-SET" + TRAINING_SET, sysevr_yooncnn_history, ['train', 'val'], 'upper left',
                        sysevr_yooncnn_loss_path,
                        sysevr_yooncnn_acc_path)

sysevr_yooncnn_metrics_path = "../graphs_reports/YOONCNN/sysevr_yooncnn_results_set" + TRAINING_SET + ".txt"
metrics_to_file("SYSEVR YOONCNN RESULTS-SET" + TRAINING_SET, sysevr_yooncnn_metrics_path, sysevr_y_test,
                sysevr_yooncnn_preds, ['0', '1'], sysevr_yooncnn_acc, BATCH_SIZE, "", OPTIMIZER)

########################################################################################################################
