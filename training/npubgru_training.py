import numpy as np  # linear algebra
import pandas as pd
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
from Models import hyperparameters, my_preprocessing
from Models.models import NPUBGRU, metrics_to_file, acc_loss_graphs_to_file, create_callbacks

import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""

CG_MAX = 50
AST_MAX = 620
SYSEVR_MAX = 500
EMBEDDING_SIZE = 100

EPOCHS = hyperparameters.EPOCHS
BATCH_SIZE = hyperparameters.BATCH_SIZE_6
DROPOUT = " "
OPTIMIZER = hyperparameters.OPTIMIZER_6
TRAINING_SET = '6'

# Set paths
asts_path = "../data/asts/all_asts.csv"
cg_path = "../data/code_gadgets/all_code_gadgets.csv"
sysevr_path = "../data/sysevr/all_standardized_sysevr.csv"

# Load data
# ASTs
ast_df = pd.read_csv(asts_path)
ast_data = ast_df['vectors'].to_list()
ast_labels = ast_df['label'].to_list()

##########################################
# Code cadgets
cg_df = pd.read_csv(cg_path)
cg_data = cg_df['code'].to_list()
# cg_labels = cg_df['label'].to_list()

back_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 1))]
forward_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 0))]

back_slices_data = back_slices_df['code'].tolist()
forward_slices_data = forward_slices_df['code'].tolist()
back_slices_labels = back_slices_df['label'].tolist()
forward_slices_labels = forward_slices_df['label'].tolist()

################################################
# Sysevr
sysevr_df = pd.read_csv(sysevr_path)
sysevr_data = sysevr_df['code'].to_list()
sysevr_labels = sysevr_df['label'].to_list()
sysevr_focus = sysevr_df['focus'].to_list()

# # Get max number of words in each dataset
# ast_max_length = max(len(ast.split(",")) for ast in ast_data)  # 5823
# cg_max_length = max(len(cg.split()) for cg in cg_data)  # 1516
# sysevr_max_length = max(len(sysevr.split()) for sysevr in sysevr_data)  # 966
#
# print("AST max :" + str(ast_max_length))
# print("Code Gadget max :" + str(cg_max_length))
# print("Sysevr max :" + str(sysevr_max_length))

# Load word embeddings
asts_w2v_path = "..\\Word2Vec\\ast_num_w2v.txt"
asts_vocab = "..\\Word2Vec\\ast_num_vocab.json"

cg_w2v_path = "..\\Word2Vec\\cg_w2v.txt"
cg_vocab = "..\\Word2Vec\\cg_vocab.json"

sysevr_w2v_path = "..\\Word2Vec\\sysevr_w2v.txt"
sysevr_vocab = "..\\Word2Vec\\sysevr_vocab.json"

ast_embeddings = open(asts_w2v_path, encoding='utf-8')
with open(asts_vocab, 'r') as a:
    words1 = a.read()
ast_words = json.loads(words1)
print('fetched ast embeddings')

cg_embeddings = open(cg_w2v_path, encoding='utf-8')
with open(cg_vocab, 'r') as b:
    words2 = b.read()
cg_words = json.loads(words2)
print('fetched cg embeddings')

sysevr_embeddings = open(sysevr_w2v_path, encoding='utf-8')
with open(sysevr_vocab, 'r') as c:
    words3 = c.read()
sysevr_words = json.loads(words3)
print('fetched sysevr embeddings')

# Create embedding dictionary
ast_emb_dict = {}
for line in ast_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        ast_emb_dict[word] = vectors
ast_embeddings.close()

cg_emb_dict = {}
for line in cg_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        cg_emb_dict[word] = vectors
cg_embeddings.close()

sysevr_emb_dict = {}
for line in sysevr_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        sysevr_emb_dict[word] = vectors
sysevr_embeddings.close()

print('asts embedding')
print(ast_emb_dict['34'])

print('cg embedding')
print(cg_emb_dict['VAR1'])

print('sysevr embedding')
print(sysevr_emb_dict['const'])

# Tokenize corpus
ast_tokenizer = Tokenizer()

cg_tokenizer = Tokenizer()
bcg_tokenizer = Tokenizer()
fcg_tokenizer = Tokenizer()

sysevr_tokenizer = Tokenizer()

print("tokenizing asts")
# Fit tokenizers
ast_tokenizer.fit_on_texts(ast_data)

print("tokenizing cgs")
bcg_tokenizer.fit_on_texts(back_slices_data)
fcg_tokenizer.fit_on_texts(forward_slices_data)

print("tokenizing sysevr")
sysevr_tokenizer.fit_on_texts(sysevr_data)

#################################################
print("creating ast sequence")
ast_sequences = my_preprocessing.ast_sequence(ast_data)

print("creating cg sequence")
bcg_sequences = bcg_tokenizer.texts_to_sequences(back_slices_data)
fcg_sequences = fcg_tokenizer.texts_to_sequences(forward_slices_data)

print("creating sysevr sequence")
sysevr_sequences = sysevr_tokenizer.texts_to_sequences(sysevr_data)

# Pad sequences to the same length
# pad asts
print("padding asts")
ast_padded_data = pad_sequences(ast_sequences, maxlen=AST_MAX, padding='post')

# pad code gadgets
print("padding cgs")
bcg_padded_data = pad_sequences(bcg_sequences, maxlen=CG_MAX, padding='pre')
fcg_padded_data = pad_sequences(fcg_sequences, maxlen=CG_MAX, padding='post')
cg_padded_data = np.concatenate((bcg_padded_data, fcg_padded_data), axis=0)

# pad sysevr
print("padding sysevr")
sysevr_padded_data = my_preprocessing.sevc_pad_sequences(sysevr_sequences, sysevr_focus, SYSEVR_MAX)

#################
# covert all labels to numpy arrays
ast_labels_np = np.asarray(ast_labels)

back_slices_labels_np = np.asarray(back_slices_labels)
forward_slices_labels_np = np.asarray(forward_slices_labels)
cg_labels_np = np.concatenate((back_slices_labels_np, forward_slices_labels_np), axis=0)

sysevr_labels_np = np.asarray(sysevr_labels)

print("shuffle data")
# shuffle ast gadgets
ids = np.random.permutation(len(ast_padded_data))
n_ast_padded_data, n_ast_labels = ast_padded_data[ids], ast_labels_np[ids]

# shuffle code gadgets
idx = np.random.permutation(len(cg_padded_data))
n_cg_padded_data, n_cg_labels = cg_padded_data[idx], cg_labels_np[idx]

# shuffle sysevr
idz = np.random.permutation(len(sysevr_padded_data))
n_sysevr_padded_data, n_sysevr_labels = sysevr_padded_data[idz], sysevr_labels_np[idz]

print("creating word indices")
# Get word indices
ast_word_index = ast_tokenizer.word_index

bs_word_index = bcg_tokenizer.word_index
fs_word_index = fcg_tokenizer.word_index
cg_word_index = {**bs_word_index, **fs_word_index}

sysevr_word_index = sysevr_tokenizer.word_index

# Create embedding matrix
ast_num_words = len(ast_word_index) + 1
cg_num_words = len(cg_word_index) + 1
sysevr_num_words = len(sysevr_word_index) + 1

ast_embedding_matrix = np.zeros((ast_num_words, EMBEDDING_SIZE))
cg_embedding_matrix = np.zeros((cg_num_words, EMBEDDING_SIZE))
sysevr_embedding_matrix = np.zeros((sysevr_num_words, EMBEDDING_SIZE))

# ast embedding matrix
print('creating ast embedding matrix')
for aword, anum in ast_word_index.items():
    if anum > ast_num_words:
        continue
    ast_embedding_vector = ast_emb_dict.get(aword)
    if ast_embedding_vector is not None:
        ast_embedding_matrix[anum] = ast_embedding_vector
    else:
        ast_embedding_matrix[anum] = np.random.randn(EMBEDDING_SIZE)

# code gadget embedding matrix
print('creating cg embedding matrix')
for cgword, cgnum in cg_word_index.items():
    if cgnum > cg_num_words:
        continue
    cg_embedding_vector = cg_emb_dict.get(cgword)
    if cg_embedding_vector is not None:
        cg_embedding_matrix[cgnum] = cg_embedding_vector
    else:
        cg_embedding_matrix[cgnum] = np.random.randn(EMBEDDING_SIZE)

# sysevr embedding matrix
print('creating sysevr embedding matrix')
for sword, snum in sysevr_word_index.items():
    if snum > sysevr_num_words:
        continue
    sysevr_embedding_vector = sysevr_emb_dict.get(sword)
    if sysevr_embedding_vector is not None:
        sysevr_embedding_matrix[snum] = sysevr_embedding_vector
    else:
        sysevr_embedding_matrix[snum] = np.random.randn(EMBEDDING_SIZE)

# split data 80, 20
print('splitting data')
ast_X_train, ast_X_test, ast_y_train, ast_y_test = train_test_split(n_ast_padded_data, n_ast_labels, test_size=0.2)
cg_X_train, cg_X_test, cg_y_train, cg_y_test = train_test_split(n_cg_padded_data, n_cg_labels, test_size=0.2)
sysevr_X_train, sysevr_X_test, sysevr_y_train, sysevr_y_test = train_test_split(n_sysevr_padded_data, n_sysevr_labels,
                                                                                test_size=0.2)
print('one-hot encoding labels')
# one hot encode labels
ast_y_train = to_categorical(ast_y_train)
ast_y_test = to_categorical(ast_y_test)

cg_y_train = to_categorical(cg_y_train)
cg_y_test = to_categorical(cg_y_test)

sysevr_y_train = to_categorical(sysevr_y_train)
sysevr_y_test = to_categorical(sysevr_y_test)

# Creating early stopping callback and Model Checkpoint
print("creating callbacks")
ast_best_model_path = "..\\saved_models\\NPUBGRU\\ast_npubgru_model_set"+TRAINING_SET+".h5"
ast_callback = create_callbacks(ast_best_model_path, 'val_loss', 'min', 2)

cg_best_model_path = "..\\saved_models\\NPUBGRU\\cg_npubgru_model_set"+TRAINING_SET+".h5"
cg_callback = create_callbacks(cg_best_model_path, 'val_loss', 'min', 2)

sysevr_best_model_path = "..\\saved_models\\NPUBGRU\\sysevr_npubgru_model_set"+TRAINING_SET+".h5"
sysevr_callback = create_callbacks(sysevr_best_model_path, 'val_loss', 'min', 2)


# Compile models
# ASTS
print('training ast_npubgru')
# ast_npubgru = NPUBGRU(ast_num_words, EMBEDDING_SIZE, ast_embedding_matrix, AST_MAX, DROPOUT)
# ast_npubgru.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
# ast_npubgru.summary()
#
# ast_npubgru_history = ast_npubgru.fit(ast_X_train, ast_y_train,
#                                       epochs=EPOCHS,
#                                       batch_size=BATCH_SIZE,
#                                       verbose=1,
#                                       validation_split=0.1,
#                                       callbacks=ast_callback,
#                                       shuffle=False)
# ast_npubgru_acc = ast_npubgru.evaluate(ast_X_test, ast_y_test, batch_size=BATCH_SIZE)
# ast_npubgru_preds = ast_npubgru.predict(ast_X_test, verbose=1)
# ast_npubgru_preds = np.argmax(ast_npubgru_preds, axis=1)
#
# ast_npubgru_loss_path = "../graphs_reports/NPUBGRU/AST_NPUBGRU_LOSS_set"+TRAINING_SET+".png"
# ast_npubgru_acc_path = "../graphs_reports/NPUBGRU/AST_NPUBGRU_ACC_set"+TRAINING_SET+".png"
# acc_loss_graphs_to_file("AST_NPUBGRU", ast_npubgru_history, ['train', 'val'], 'upper left', ast_npubgru_loss_path,
#                         ast_npubgru_acc_path)
#
# ast_npubgru_metrics_path = "../graphs_reports/NPUBGRU/ast_npubgru_results_set"+TRAINING_SET+".txt"
# metrics_to_file("AST NPUBGRU RESULTS-SET"+TRAINING_SET, ast_npubgru_metrics_path, ast_y_test, ast_npubgru_preds, ['0', '1'],
#                 ast_npubgru_acc, BATCH_SIZE, DROPOUT, OPTIMIZER)
########################################################################################################################

# CODE GADGETS
print('training cg_npubgru')
cg_npubgru = NPUBGRU(cg_num_words, EMBEDDING_SIZE, cg_embedding_matrix, CG_MAX, DROPOUT)
cg_npubgru.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
cg_npubgru.summary()

cg_npubgru_history = cg_npubgru.fit(cg_X_train, cg_y_train,
                                    epochs=EPOCHS,
                                    batch_size=BATCH_SIZE,
                                    verbose=1,
                                    validation_split=0.1,
                                    callbacks=cg_callback,
                                    shuffle=False)
cg_npubgru_acc = cg_npubgru.evaluate(cg_X_test, cg_y_test, batch_size=BATCH_SIZE)
cg_npubgru_preds = cg_npubgru.predict(cg_X_test, verbose=1)
cg_npubgru_preds = np.argmax(cg_npubgru_preds, axis=1)

cg_npubgru_loss_path = "../graphs_reports/NPUBGRU/CG_NPUBGRU_LOSS_set"+TRAINING_SET+".png"
cg_npubgru_acc_path = "../graphs_reports/NPUBGRU/CG_NPUBGRU_ACC_set"+TRAINING_SET+".png"
acc_loss_graphs_to_file("CG_NPUBGRU", cg_npubgru_history, ['train', 'val'], 'upper left', cg_npubgru_loss_path,
                        cg_npubgru_acc_path)

cg_npubgru_metrics_path = "../graphs_reports/NPUBGRU/cg_npubgru_results_set"+TRAINING_SET+".txt"
metrics_to_file("CG NPUBGRU RESULTS-SET"+TRAINING_SET, cg_npubgru_metrics_path, cg_y_test, cg_npubgru_preds, ['0', '1'], cg_npubgru_acc,
                BATCH_SIZE, DROPOUT, OPTIMIZER)
########################################################################################################################

# SYSEVR
print('training sysevr_npubgru')
sysevr_npubgru = NPUBGRU(sysevr_num_words, EMBEDDING_SIZE, sysevr_embedding_matrix, SYSEVR_MAX, DROPOUT)
sysevr_npubgru.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
sysevr_npubgru.summary()

sysevr_npubgru_history = sysevr_npubgru.fit(sysevr_X_train, sysevr_y_train,
                                            epochs=EPOCHS,
                                            batch_size=BATCH_SIZE,
                                            verbose=1,
                                            validation_split=0.1,
                                            callbacks=sysevr_callback,
                                            shuffle=False)
sysevr_npubgru_acc = sysevr_npubgru.evaluate(sysevr_X_test, sysevr_y_test, batch_size=BATCH_SIZE)
sysevr_npubgru_preds = sysevr_npubgru.predict(sysevr_X_test, verbose=1)
sysevr_npubgru_preds = np.argmax(sysevr_npubgru_preds, axis=1)

sysevr_npubgru_loss_path = "../graphs_reports/NPUBGRU/SYSEVR_NPUBGRU_LOSS_set"+TRAINING_SET+".png"
sysevr_npubgru_acc_path = "../graphs_reports/NPUBGRU/SYSEVR_NPUBGRU_ACC_set"+TRAINING_SET+".png"
acc_loss_graphs_to_file("SYSEVR_NPUBGRU", sysevr_npubgru_history, ['train', 'val'], 'upper left',
                        sysevr_npubgru_loss_path,
                        sysevr_npubgru_acc_path)

sysevr_npubgru_metrics_path = "../graphs_reports/NPUBGRU/sysevr_npubgru_results_set"+TRAINING_SET+".txt"
metrics_to_file("SYSEVR NPUBGRU RESULTS-SET"+TRAINING_SET, sysevr_npubgru_metrics_path, sysevr_y_test, sysevr_npubgru_preds, ['0', '1'],
                sysevr_npubgru_acc, BATCH_SIZE, DROPOUT, OPTIMIZER)

########################################################################################################################
