import numpy as np
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.pipeline import Pipeline


# Pad sevcs according SYSEVR paper
def sevc_pad_sequences(seqs, mfocus, maxlen):
    new_array = []
    vector_dim = 1
    fill_0 = 0 * vector_dim

    for i in range(len(seqs)):
        if len(seqs[i]) < maxlen:
            seqs[i] = seqs[i] + [fill_0] * (maxlen - len(seqs[i]))
            new_array.append(seqs[i])

        elif len(seqs[i]) == maxlen:
            new_array.append(seqs[i])

        else:
            startpoint = int(mfocus[i] - round(maxlen / 2.0))
            endpoint = int(startpoint + maxlen)
            if startpoint < 0:
                startpoint = 0
                endpoint = maxlen
            if endpoint >= len(seqs[i]):
                startpoint = -maxlen
                endpoint = None
            new_array.append(seqs[i][startpoint:endpoint])

    return np.asarray(new_array)


# Create AST sequences for padding
def ast_sequence(data):
    array = []

    for d in data:
        token = d.split(',')
        ntoken = list(map(int, token))
        array.append(ntoken)

    return array


# Over-sample vulnerable class by 40% and under-sample non vulnerable class by 20%
# Apply smote data-balancing techniques
def smote(train_data, test_labels, os, us):
    over = SMOTE(sampling_strategy=os)
    under = RandomUnderSampler(sampling_strategy=us)
    steps = [('o', over), ('u', under)]
    pipeline = Pipeline(steps=steps)
    x, y = pipeline.fit_resample(train_data, test_labels)
    return x, y


# Apply random oversampling techniques
def over_sample(train_data, test_labels, os):
    if os == 0:
        print('No oversampling')
        return train_data, test_labels

    else:
        over = RandomOverSampler(sampling_strategy=os)
        steps = [('o', over)]
        pipeline = Pipeline(steps=steps)
        x, y = pipeline.fit_resample(train_data, test_labels)
        return x, y
