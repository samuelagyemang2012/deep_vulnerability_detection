# Datasets
Link to datasets and trained word2vec models\
https://drive.google.com/open?id=1nKAecH-lPkM5K5hafiMhC6Rtl_u-Q0ng

# Folders
data: Contains 3 folders with 3 different datasets (ast, code_gadgets, sysevr).

Word2Vec: Contains the trained Word2Vec models for each dataset with embedding sizes of 100 and 300.

Models: Contains the definition of the deep learning models (VDBLSTM, NPUBGRU, HAN, YoonCNN, VANILLA), the hyper-parameters used in training, and some other functions.

graphs_reports: Accuracy graphs, Loss graphs, and training report are saved in this folder.

training: Contains training scripts for each deep learning model.

# How to run
1. Extract the zipped file in the link above.
2. Put the **data** and **Word2Vec** folders in the project folder. Also create a **saved_models** folder in project folder.
3. Select any of the training scripts in the training folder.
4. Set patience for training (default is 10), validation split(default is 0.1) and the test size(default is 0.2)
5. Set your hyper-parameters i.e \
EPOCHS = (default is 200. Early stopping callback is used to determine the right number of training epochs),\
BATCH_SIZE (16, 32, 64),\
OPTIMIZER (adam, rmsprop)\
All hyperparameters used can be found in the Model/hyperparameters.py file.

5. Set the max length of sequences for each dataset, embedding size (default is 100) and training set number.
NB: To train with an embedding size of 300, set the embedding size to 300 and modify the filenames in the word embeddings path for all datasets.\
eg. EMBEDDING_SIZE=300\
    cg_w2v_path = ".\\Word2Vec\\cg_w2v_300.txt"\
    cg_vocab = "..\\Word2Vec\\cg_vocab_300.json"\

7. Run the script. The results (accuracy and loss graph, confusion matrix, classification report, recall, precision...etc) will saved in the graph_reports folder. eg. CG_HAN_ACC_set1.png, CG_HAN_LOSS_set1.png, CG_HAN_results_set1.txt) The trained model will be saved in the saved_models folder created.
