import numpy as np  # linear algebra
import pandas as pd
import json
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils.np_utils import to_categorical
from sklearn.model_selection import train_test_split
from Models import hyperparameters, my_preprocessing
from Models.models import VDBLSTM, metrics_to_file, acc_loss_graphs_to_file, create_callbacks, append_metrics, \
    get_averages, clear_lists

import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""

CG_MAX = 50
AST_MAX = 850
SYSEVR_MAX = 500
EMBEDDING_SIZE = 100
RUNS = 10

#############################################################
accs = []
fprs = []
fnrs = []
recalls = []
precs = []
f1s = []
epchs = []
#############################################################

EPOCHS = hyperparameters.EPOCHS
BATCH_SIZE = hyperparameters.BATCH_SIZE
DROPOUT = " "
OPTIMIZER = hyperparameters.OPTIMIZER
TRAINING_SET = '1'

# Set paths
asts_path = "../data/asts/all_asts.csv"
cg_path = "../data/code_gadgets/all_code_gadgets.csv"
sysevr_path = "../data/sysevr/all_standardized_sysevr.csv"

# Load data
# ASTs
ast_df = pd.read_csv(asts_path)
ast_data = ast_df['vectors'].to_list()
ast_labels = ast_df['label'].to_list()

##########################################
# Code cadgets
cg_df = pd.read_csv(cg_path)
cg_data = cg_df['code'].to_list()
# cg_labels = cg_df['label'].to_list()

back_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 1))]
forward_slices_df = cg_df.loc[((cg_df['is_backward_slice'] == 0))]

back_slices_data = back_slices_df['code'].tolist()
forward_slices_data = forward_slices_df['code'].tolist()
back_slices_labels = back_slices_df['label'].tolist()
forward_slices_labels = forward_slices_df['label'].tolist()

################################################
# Sysevr
sysevr_df = pd.read_csv(sysevr_path)
sysevr_data = sysevr_df['code'].to_list()
sysevr_labels = sysevr_df['label'].to_list()
sysevr_focus = sysevr_df['focus'].to_list()

# # Get max number of words in each dataset
# ast_max_length = max(len(ast.split(",")) for ast in ast_data)  # 5823
# cg_max_length = max(len(cg.split()) for cg in cg_data)  # 1516
# sysevr_max_length = max(len(sysevr.split()) for sysevr in sysevr_data)  # 966
#
# print("AST max :" + str(ast_max_length))
# print("Code Gadget max :" + str(cg_max_length))
# print("Sysevr max :" + str(sysevr_max_length))

# Load word embeddings
asts_w2v_path = "..\\Word2Vec\\ast_num_w2v.txt"
asts_vocab = "..\\Word2Vec\\ast_num_vocab.json"

cg_w2v_path = "..\\Word2Vec\\cg_w2v.txt"
cg_vocab = "..\\Word2Vec\\cg_vocab.json"

sysevr_w2v_path = "..\\Word2Vec\\sysevr_w2v.txt"
sysevr_vocab = "..\\Word2Vec\\sysevr_vocab.json"

ast_embeddings = open(asts_w2v_path, encoding='utf-8')
with open(asts_vocab, 'r') as a:
    words1 = a.read()
ast_words = json.loads(words1)
print('fetched ast embeddings')

cg_embeddings = open(cg_w2v_path, encoding='utf-8')
with open(cg_vocab, 'r') as b:
    words2 = b.read()
cg_words = json.loads(words2)
print('fetched cg embeddings')

sysevr_embeddings = open(sysevr_w2v_path, encoding='utf-8')
with open(sysevr_vocab, 'r') as c:
    words3 = c.read()
sysevr_words = json.loads(words3)
print('fetched sysevr embeddings')

# Create embedding dictionary
ast_emb_dict = {}
for line in ast_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        ast_emb_dict[word] = vectors
ast_embeddings.close()

cg_emb_dict = {}
for line in cg_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        cg_emb_dict[word] = vectors
cg_embeddings.close()

sysevr_emb_dict = {}
for line in sysevr_embeddings:
    values = line.split()
    if len(values) != 2:
        word = values[0]
        vectors = np.asarray(values[1:])
        sysevr_emb_dict[word] = vectors
sysevr_embeddings.close()

print('asts embedding')
print(ast_emb_dict['34'])

print('cg embedding')
print(cg_emb_dict['VAR1'])

print('sysevr embedding')
print(sysevr_emb_dict['const'])

# Tokenize corpus
ast_tokenizer = Tokenizer()

cg_tokenizer = Tokenizer()
bcg_tokenizer = Tokenizer()
fcg_tokenizer = Tokenizer()

sysevr_tokenizer = Tokenizer()

print("tokenizing asts")
# Fit tokenizers
ast_tokenizer.fit_on_texts(ast_data)

print("tokenizing cgs")
bcg_tokenizer.fit_on_texts(back_slices_data)
fcg_tokenizer.fit_on_texts(forward_slices_data)

print("tokenizing sysevr")
sysevr_tokenizer.fit_on_texts(sysevr_data)

#################################################
print("creating ast sequence")
ast_sequences = my_preprocessing.ast_sequence(ast_data)

print("creating cg sequence")
bcg_sequences = bcg_tokenizer.texts_to_sequences(back_slices_data)
fcg_sequences = fcg_tokenizer.texts_to_sequences(forward_slices_data)

print("creating sysevr sequence")
sysevr_sequences = sysevr_tokenizer.texts_to_sequences(sysevr_data)

# Pad sequences to the same length
# pad asts
print("padding asts")
ast_padded_data = pad_sequences(ast_sequences, maxlen=AST_MAX, padding='post')

# pad code gadgets
print("padding cgs")
bcg_padded_data = pad_sequences(bcg_sequences, maxlen=CG_MAX, padding='pre')
fcg_padded_data = pad_sequences(fcg_sequences, maxlen=CG_MAX, padding='post')
cg_padded_data = np.concatenate((bcg_padded_data, fcg_padded_data), axis=0)

# pad sysevr
print("padding sysevr")
sysevr_padded_data = my_preprocessing.sevc_pad_sequences(sysevr_sequences, sysevr_focus, SYSEVR_MAX)

#################
# covert all labels to numpy arrays
ast_labels_np = np.asarray(ast_labels)

back_slices_labels_np = np.asarray(back_slices_labels)
forward_slices_labels_np = np.asarray(forward_slices_labels)
cg_labels_np = np.concatenate((back_slices_labels_np, forward_slices_labels_np), axis=0)

sysevr_labels_np = np.asarray(sysevr_labels)

print("shuffle data")
# shuffle ast gadgets
ids = np.random.permutation(len(ast_padded_data))
n_ast_padded_data, n_ast_labels = ast_padded_data[ids], ast_labels_np[ids]

# shuffle code gadgets
idx = np.random.permutation(len(cg_padded_data))
n_cg_padded_data, n_cg_labels = cg_padded_data[idx], cg_labels_np[idx]

# shuffle sysevr
idz = np.random.permutation(len(sysevr_padded_data))
n_sysevr_padded_data, n_sysevr_labels = sysevr_padded_data[idz], sysevr_labels_np[idz]

print("creating word indices")
# Get word indices
ast_word_index = ast_tokenizer.word_index

bs_word_index = bcg_tokenizer.word_index
fs_word_index = fcg_tokenizer.word_index
cg_word_index = {**bs_word_index, **fs_word_index}

sysevr_word_index = sysevr_tokenizer.word_index

# Create embedding matrix
ast_num_words = len(ast_word_index) + 1
cg_num_words = len(cg_word_index) + 1
sysevr_num_words = len(sysevr_word_index) + 1

ast_embedding_matrix = np.zeros((ast_num_words, EMBEDDING_SIZE))
cg_embedding_matrix = np.zeros((cg_num_words, EMBEDDING_SIZE))
sysevr_embedding_matrix = np.zeros((sysevr_num_words, EMBEDDING_SIZE))

# ast embedding matrix
print('creating ast embedding matrix')
for aword, anum in ast_word_index.items():
    if anum > ast_num_words:
        continue
    ast_embedding_vector = ast_emb_dict.get(aword)
    if ast_embedding_vector is not None:
        ast_embedding_matrix[anum] = ast_embedding_vector
    else:
        ast_embedding_matrix[anum] = np.random.randn(EMBEDDING_SIZE)

# code gadget embedding matrix
print('creating cg embedding matrix')
for cgword, cgnum in cg_word_index.items():
    if cgnum > cg_num_words:
        continue
    cg_embedding_vector = cg_emb_dict.get(cgword)
    if cg_embedding_vector is not None:
        cg_embedding_matrix[cgnum] = cg_embedding_vector
    else:
        cg_embedding_matrix[cgnum] = np.random.randn(EMBEDDING_SIZE)

# sysevr embedding matrix
print('creating sysevr embedding matrix')
for sword, snum in sysevr_word_index.items():
    if snum > sysevr_num_words:
        continue
    sysevr_embedding_vector = sysevr_emb_dict.get(sword)
    if sysevr_embedding_vector is not None:
        sysevr_embedding_matrix[snum] = sysevr_embedding_vector
    else:
        sysevr_embedding_matrix[snum] = np.random.randn(EMBEDDING_SIZE)

# split data 80, 20
print('splitting data')
ast_X_train, ast_X_test, ast_y_train, ast_y_test = train_test_split(n_ast_padded_data, n_ast_labels, test_size=0.2)
cg_X_train, cg_X_test, cg_y_train, cg_y_test = train_test_split(n_cg_padded_data, n_cg_labels, test_size=0.2)
sysevr_X_train, sysevr_X_test, sysevr_y_train, sysevr_y_test = train_test_split(n_sysevr_padded_data, n_sysevr_labels,
                                                                                test_size=0.2)
print('one-hot encoding labels')
# one hot encode labels
ast_y_train = to_categorical(ast_y_train)
ast_y_test = to_categorical(ast_y_test)

cg_y_train = to_categorical(cg_y_train)
cg_y_test = to_categorical(cg_y_test)

sysevr_y_train = to_categorical(sysevr_y_train)
sysevr_y_test = to_categorical(sysevr_y_test)

# Creating early stopping callback and Model Checkpoint
print("creating callbacks")
ast_best_model_path = "..\\saved_models\\VDBLSTM\\ast_vdblstm_model_set" + TRAINING_SET + ".h5"
ast_callback = create_callbacks(ast_best_model_path, 'val_loss', 'min', 1)

cg_best_model_path = "..\\saved_models\\VDBLSTM\\cg_vdblstm_model_set" + TRAINING_SET + ".h5"
cg_callback = create_callbacks(cg_best_model_path, 'val_loss', 'min', 1)

sysevr_best_model_path = "..\\saved_models\\VDBLSTM\\sysevr_vdblstm_model_set" + TRAINING_SET + ".h5"
sysevr_callback = create_callbacks(sysevr_best_model_path, 'val_loss', 'min', 1)

# Compile models
# ASTS
print('training ast_vdblstm')
for i in range(0, RUNS):
    ast_vdblstm = VDBLSTM(ast_num_words, EMBEDDING_SIZE, ast_embedding_matrix, AST_MAX, DROPOUT)
    ast_vdblstm.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
    ast_vdblstm.summary()

    ast_vdblstm_history = ast_vdblstm.fit(ast_X_train, ast_y_train,
                                          epochs=EPOCHS,
                                          batch_size=BATCH_SIZE,
                                          verbose=2,
                                          validation_split=0.1,
                                          callbacks=ast_callback,
                                          shuffle=False)
    ast_vdblstm_acc = ast_vdblstm.evaluate(ast_X_test, ast_y_test, batch_size=BATCH_SIZE)
    ast_vdblstm_preds = ast_vdblstm.predict(ast_X_test, verbose=0)
    ast_vdblstm_preds = np.argmax(ast_vdblstm_preds, axis=1)

    ast_vdblstm_loss_path = "../graphs_reports/VDBLSTM/AST_VDBLSTM_LOSS_set" + TRAINING_SET + ".png"
    ast_vdblstm_acc_path = "../graphs_reports/VDBLSTM/AST_VDBLSTM_ACC_set" + TRAINING_SET + ".png"
    acc_loss_graphs_to_file("AST_VDBLSTM", ast_vdblstm_history, ['train', 'val'], 'upper left', ast_vdblstm_loss_path,
                            ast_vdblstm_acc_path)

    ast_vdblstm_metrics_path = "../graphs_reports/VDBLSTM/ast_vdblstm_results_set" + TRAINING_SET + ".txt"
    metrics_to_file("AST VDBLSTM RESULTS-SET" + TRAINING_SET, ast_vdblstm_metrics_path, ast_y_test, ast_vdblstm_preds,
                    ['0', '1'],
                    ast_vdblstm_acc, BATCH_SIZE, DROPOUT, OPTIMIZER)

    append_metrics(ast_y_test, ast_vdblstm_preds, accs, fnrs, fprs, precs, recalls, f1s, epchs, ast_vdblstm_history)

print('AST results')
get_averages(accs, fnrs, fprs, precs, recalls, f1s, epchs)
clear_lists(accs, fnrs, fprs, precs, recalls, f1s, epchs)
print('')
########################################################################################################################
# CODE GADGETS
print('training cg_vdblstm')
for i in range(0, RUNS):
    cg_vdblstm = VDBLSTM(cg_num_words, EMBEDDING_SIZE, cg_embedding_matrix, CG_MAX, DROPOUT)
    cg_vdblstm.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
    cg_vdblstm.summary()

    cg_vdblstm_history = cg_vdblstm.fit(cg_X_train, cg_y_train,
                                    epochs=EPOCHS,
                                    batch_size=BATCH_SIZE,
                                    verbose=2,
                                    validation_split=0.1,
                                    callbacks=cg_callback,
                                    shuffle=False)
    cg_vdblstm_acc = cg_vdblstm.evaluate(cg_X_test, cg_y_test, batch_size=BATCH_SIZE)
    cg_vdblstm_preds = cg_vdblstm.predict(cg_X_test, verbose=1)
    cg_vdblstm_preds = np.argmax(cg_vdblstm_preds, axis=1)

    cg_vdblstm_loss_path = "../graphs_reports/VDBLSTM/CG_VDBLSTM_LOSS_set"+TRAINING_SET+".png"
    cg_vdblstm_acc_path = "../graphs_reports/VDBLSTM/CG_VDBLSTM_ACC_set"+TRAINING_SET+".png"
    acc_loss_graphs_to_file("CG_VDBLSTM", cg_vdblstm_history, ['train', 'val'], 'upper left', cg_vdblstm_loss_path,
                        cg_vdblstm_acc_path)

    cg_vdblstm_metrics_path = "../graphs_reports/VDBLSTM/cg_vdblstm_results_set"+TRAINING_SET+".txt"
    metrics_to_file("CG VDBLSTM RESULTS-SET"+TRAINING_SET, cg_vdblstm_metrics_path, cg_y_test, cg_vdblstm_preds, ['0', '1'], cg_vdblstm_acc,
                BATCH_SIZE, DROPOUT, OPTIMIZER)
    append_metrics(cg_y_test, cg_vdblstm_preds, accs, fnrs, fprs, precs, recalls, f1s, epchs, cg_vdblstm_history)

print('CG results')
get_averages(accs, fnrs, fprs, precs, recalls, f1s, epchs)
clear_lists(accs, fnrs, fprs, precs, recalls, f1s, epchs)
print('')

########################################################################################################################

# SYSEVR
print('training sysevr_vdblstm')
for i in range(0, RUNS):
    sysevr_vdblstm = VDBLSTM(sysevr_num_words, EMBEDDING_SIZE, sysevr_embedding_matrix, SYSEVR_MAX, DROPOUT)
    sysevr_vdblstm.compile(loss='binary_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])
    sysevr_vdblstm.summary()

    sysevr_vdblstm_history = sysevr_vdblstm.fit(sysevr_X_train, sysevr_y_train,
                                            epochs=EPOCHS,
                                            batch_size=BATCH_SIZE,
                                            verbose=2,
                                            validation_split=0.1,
                                            callbacks=sysevr_callback,
                                            shuffle=False)
    sysevr_vdblstm_acc = sysevr_vdblstm.evaluate(sysevr_X_test, sysevr_y_test, batch_size=BATCH_SIZE)
    sysevr_vdblstm_preds = sysevr_vdblstm.predict(sysevr_X_test, verbose=1)
    sysevr_vdblstm_preds = np.argmax(sysevr_vdblstm_preds, axis=1)

    sysevr_vdblstm_loss_path = "../graphs_reports/VDBLSTM/SYSEVR_VDBLSTM_LOSS_set" + TRAINING_SET + ".png"
    sysevr_vdblstm_acc_path = "../graphs_reports/VDBLSTM/SYSEVR_VDBLSTM_ACC_set" + TRAINING_SET + ".png"
    acc_loss_graphs_to_file("SYSEVR_VDBLSTM", sysevr_vdblstm_history, ['train', 'val'], 'upper left',
                        sysevr_vdblstm_loss_path,
                        sysevr_vdblstm_acc_path)

    sysevr_vdblstm_metrics_path = "../graphs_reports/VDBLSTM/sysevr_vdblstm_results_set" + TRAINING_SET + ".txt"
    metrics_to_file("SYSEVR VDBLSTM RESULTS-SET" + TRAINING_SET, sysevr_vdblstm_metrics_path, sysevr_y_test,
                sysevr_vdblstm_preds, ['0', '1'],
                sysevr_vdblstm_acc, BATCH_SIZE, DROPOUT, OPTIMIZER)

    append_metrics(sysevr_y_test, sysevr_vdblstm_preds, accs, fnrs, fprs, precs, recalls, f1s, epchs,
                   sysevr_vdblstm_history)

print('SYSEVR results')
get_averages(accs, fnrs, fprs, precs, recalls, f1s, epchs)
clear_lists(accs, fnrs, fprs, precs, recalls, f1s, epchs)
print('')

########################################################################################################################
